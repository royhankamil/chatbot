{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.01984783327820046,
  "eval_steps": 500,
  "global_step": 60,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0003307972213033411,
      "grad_norm": 0.780792236328125,
      "learning_rate": 4e-05,
      "loss": 2.0376,
      "step": 1
    },
    {
      "epoch": 0.0006615944426066821,
      "grad_norm": 0.7283021211624146,
      "learning_rate": 8e-05,
      "loss": 1.8869,
      "step": 2
    },
    {
      "epoch": 0.0009923916639100231,
      "grad_norm": 0.5777793526649475,
      "learning_rate": 0.00012,
      "loss": 1.8533,
      "step": 3
    },
    {
      "epoch": 0.0013231888852133643,
      "grad_norm": 0.7198808789253235,
      "learning_rate": 0.00016,
      "loss": 1.8486,
      "step": 4
    },
    {
      "epoch": 0.0016539861065167053,
      "grad_norm": 0.6614933609962463,
      "learning_rate": 0.0002,
      "loss": 1.7901,
      "step": 5
    },
    {
      "epoch": 0.0019847833278200462,
      "grad_norm": 0.8984084129333496,
      "learning_rate": 0.00019636363636363636,
      "loss": 1.4002,
      "step": 6
    },
    {
      "epoch": 0.002315580549123387,
      "grad_norm": 0.9115089774131775,
      "learning_rate": 0.00019272727272727274,
      "loss": 1.3821,
      "step": 7
    },
    {
      "epoch": 0.0026463777704267286,
      "grad_norm": 0.8539198637008667,
      "learning_rate": 0.0001890909090909091,
      "loss": 1.281,
      "step": 8
    },
    {
      "epoch": 0.0029771749917300696,
      "grad_norm": 1.4723567962646484,
      "learning_rate": 0.00018545454545454545,
      "loss": 1.3036,
      "step": 9
    },
    {
      "epoch": 0.0033079722130334105,
      "grad_norm": 0.5971625447273254,
      "learning_rate": 0.00018181818181818183,
      "loss": 1.0872,
      "step": 10
    },
    {
      "epoch": 0.0036387694343367515,
      "grad_norm": 0.7339792847633362,
      "learning_rate": 0.0001781818181818182,
      "loss": 1.0824,
      "step": 11
    },
    {
      "epoch": 0.0039695666556400925,
      "grad_norm": 0.5826168656349182,
      "learning_rate": 0.00017454545454545454,
      "loss": 1.1,
      "step": 12
    },
    {
      "epoch": 0.004300363876943433,
      "grad_norm": 0.5743685364723206,
      "learning_rate": 0.0001709090909090909,
      "loss": 1.1322,
      "step": 13
    },
    {
      "epoch": 0.004631161098246774,
      "grad_norm": 0.6037015914916992,
      "learning_rate": 0.00016727272727272728,
      "loss": 1.0931,
      "step": 14
    },
    {
      "epoch": 0.004961958319550115,
      "grad_norm": 0.6416943073272705,
      "learning_rate": 0.00016363636363636366,
      "loss": 1.0818,
      "step": 15
    },
    {
      "epoch": 0.005292755540853457,
      "grad_norm": 0.5773216485977173,
      "learning_rate": 0.00016,
      "loss": 1.032,
      "step": 16
    },
    {
      "epoch": 0.005623552762156798,
      "grad_norm": 0.5522733330726624,
      "learning_rate": 0.00015636363636363637,
      "loss": 0.9082,
      "step": 17
    },
    {
      "epoch": 0.005954349983460139,
      "grad_norm": 0.5854039788246155,
      "learning_rate": 0.00015272727272727275,
      "loss": 0.9619,
      "step": 18
    },
    {
      "epoch": 0.00628514720476348,
      "grad_norm": 0.6135753989219666,
      "learning_rate": 0.0001490909090909091,
      "loss": 1.0304,
      "step": 19
    },
    {
      "epoch": 0.006615944426066821,
      "grad_norm": 0.6193206310272217,
      "learning_rate": 0.00014545454545454546,
      "loss": 0.9584,
      "step": 20
    },
    {
      "epoch": 0.006946741647370162,
      "grad_norm": 0.6694983839988708,
      "learning_rate": 0.00014181818181818184,
      "loss": 0.9497,
      "step": 21
    },
    {
      "epoch": 0.007277538868673503,
      "grad_norm": 0.5717270970344543,
      "learning_rate": 0.0001381818181818182,
      "loss": 0.8768,
      "step": 22
    },
    {
      "epoch": 0.007608336089976844,
      "grad_norm": 0.5697851181030273,
      "learning_rate": 0.00013454545454545455,
      "loss": 0.9042,
      "step": 23
    },
    {
      "epoch": 0.007939133311280185,
      "grad_norm": 0.619428277015686,
      "learning_rate": 0.00013090909090909093,
      "loss": 1.0411,
      "step": 24
    },
    {
      "epoch": 0.008269930532583526,
      "grad_norm": 0.5969433188438416,
      "learning_rate": 0.00012727272727272728,
      "loss": 0.8202,
      "step": 25
    },
    {
      "epoch": 0.008600727753886867,
      "grad_norm": 0.5843127965927124,
      "learning_rate": 0.00012363636363636364,
      "loss": 0.9667,
      "step": 26
    },
    {
      "epoch": 0.008931524975190208,
      "grad_norm": 0.6845918893814087,
      "learning_rate": 0.00012,
      "loss": 0.8755,
      "step": 27
    },
    {
      "epoch": 0.009262322196493549,
      "grad_norm": 0.7502675652503967,
      "learning_rate": 0.00011636363636363636,
      "loss": 0.8728,
      "step": 28
    },
    {
      "epoch": 0.00959311941779689,
      "grad_norm": 0.6143320798873901,
      "learning_rate": 0.00011272727272727272,
      "loss": 0.8931,
      "step": 29
    },
    {
      "epoch": 0.00992391663910023,
      "grad_norm": 0.7283344864845276,
      "learning_rate": 0.00010909090909090909,
      "loss": 0.9335,
      "step": 30
    },
    {
      "epoch": 0.010254713860403573,
      "grad_norm": 0.6161403656005859,
      "learning_rate": 0.00010545454545454545,
      "loss": 0.9005,
      "step": 31
    },
    {
      "epoch": 0.010585511081706914,
      "grad_norm": 0.6612910032272339,
      "learning_rate": 0.00010181818181818181,
      "loss": 0.7734,
      "step": 32
    },
    {
      "epoch": 0.010916308303010255,
      "grad_norm": 0.7056105136871338,
      "learning_rate": 9.818181818181818e-05,
      "loss": 0.8198,
      "step": 33
    },
    {
      "epoch": 0.011247105524313596,
      "grad_norm": 0.6109590530395508,
      "learning_rate": 9.454545454545455e-05,
      "loss": 0.7955,
      "step": 34
    },
    {
      "epoch": 0.011577902745616937,
      "grad_norm": 0.5874136686325073,
      "learning_rate": 9.090909090909092e-05,
      "loss": 0.8377,
      "step": 35
    },
    {
      "epoch": 0.011908699966920278,
      "grad_norm": 0.5412797927856445,
      "learning_rate": 8.727272727272727e-05,
      "loss": 0.8023,
      "step": 36
    },
    {
      "epoch": 0.01223949718822362,
      "grad_norm": 0.5653288960456848,
      "learning_rate": 8.363636363636364e-05,
      "loss": 0.7726,
      "step": 37
    },
    {
      "epoch": 0.01257029440952696,
      "grad_norm": 0.4794739782810211,
      "learning_rate": 8e-05,
      "loss": 0.7478,
      "step": 38
    },
    {
      "epoch": 0.012901091630830301,
      "grad_norm": 0.6243316531181335,
      "learning_rate": 7.636363636363637e-05,
      "loss": 0.7922,
      "step": 39
    },
    {
      "epoch": 0.013231888852133642,
      "grad_norm": 0.5752009749412537,
      "learning_rate": 7.272727272727273e-05,
      "loss": 0.7443,
      "step": 40
    },
    {
      "epoch": 0.013562686073436983,
      "grad_norm": 0.5497546792030334,
      "learning_rate": 6.90909090909091e-05,
      "loss": 0.8145,
      "step": 41
    },
    {
      "epoch": 0.013893483294740324,
      "grad_norm": 0.4994545876979828,
      "learning_rate": 6.545454545454546e-05,
      "loss": 0.7967,
      "step": 42
    },
    {
      "epoch": 0.014224280516043665,
      "grad_norm": 0.5174778699874878,
      "learning_rate": 6.181818181818182e-05,
      "loss": 0.9374,
      "step": 43
    },
    {
      "epoch": 0.014555077737347006,
      "grad_norm": 0.5370336771011353,
      "learning_rate": 5.818181818181818e-05,
      "loss": 0.7625,
      "step": 44
    },
    {
      "epoch": 0.014885874958650347,
      "grad_norm": 0.5109252333641052,
      "learning_rate": 5.4545454545454546e-05,
      "loss": 0.6765,
      "step": 45
    },
    {
      "epoch": 0.015216672179953688,
      "grad_norm": 0.5897008776664734,
      "learning_rate": 5.090909090909091e-05,
      "loss": 0.8407,
      "step": 46
    },
    {
      "epoch": 0.015547469401257029,
      "grad_norm": 0.5286211967468262,
      "learning_rate": 4.7272727272727275e-05,
      "loss": 0.7983,
      "step": 47
    },
    {
      "epoch": 0.01587826662256037,
      "grad_norm": 0.5744078755378723,
      "learning_rate": 4.3636363636363636e-05,
      "loss": 0.6964,
      "step": 48
    },
    {
      "epoch": 0.01620906384386371,
      "grad_norm": 0.6074844002723694,
      "learning_rate": 4e-05,
      "loss": 0.836,
      "step": 49
    },
    {
      "epoch": 0.016539861065167052,
      "grad_norm": 0.6297905445098877,
      "learning_rate": 3.6363636363636364e-05,
      "loss": 0.8374,
      "step": 50
    },
    {
      "epoch": 0.016870658286470393,
      "grad_norm": 0.5619328618049622,
      "learning_rate": 3.272727272727273e-05,
      "loss": 0.7541,
      "step": 51
    },
    {
      "epoch": 0.017201455507773734,
      "grad_norm": 0.5315916538238525,
      "learning_rate": 2.909090909090909e-05,
      "loss": 0.7502,
      "step": 52
    },
    {
      "epoch": 0.017532252729077075,
      "grad_norm": 0.5198027491569519,
      "learning_rate": 2.5454545454545454e-05,
      "loss": 0.7517,
      "step": 53
    },
    {
      "epoch": 0.017863049950380416,
      "grad_norm": 0.5251091122627258,
      "learning_rate": 2.1818181818181818e-05,
      "loss": 0.6636,
      "step": 54
    },
    {
      "epoch": 0.018193847171683757,
      "grad_norm": 0.5265955328941345,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 0.7016,
      "step": 55
    },
    {
      "epoch": 0.018524644392987098,
      "grad_norm": 0.5418963432312012,
      "learning_rate": 1.4545454545454545e-05,
      "loss": 0.6929,
      "step": 56
    },
    {
      "epoch": 0.01885544161429044,
      "grad_norm": 0.6988213062286377,
      "learning_rate": 1.0909090909090909e-05,
      "loss": 0.9035,
      "step": 57
    },
    {
      "epoch": 0.01918623883559378,
      "grad_norm": 0.5114436745643616,
      "learning_rate": 7.272727272727272e-06,
      "loss": 0.7468,
      "step": 58
    },
    {
      "epoch": 0.01951703605689712,
      "grad_norm": 0.5787211656570435,
      "learning_rate": 3.636363636363636e-06,
      "loss": 0.7264,
      "step": 59
    },
    {
      "epoch": 0.01984783327820046,
      "grad_norm": 0.5271362662315369,
      "learning_rate": 0.0,
      "loss": 0.7516,
      "step": 60
    }
  ],
  "logging_steps": 1,
  "max_steps": 60,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4458478131953664.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
